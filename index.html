<!DOCTYPE html>
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta name="description"
        content="We propose a novel method that enhances jailbreak efficiency through adaptive strategy mining, leveraging unique vulnerabilities across different models and scenarios.">
    <meta name="keywords" content="Jailbreak, LLMs, Adversarial Prompt, Red Teaming">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Can Small Language Model Reliably Resist Jailbreak Attack? A Comprehensive Evaluation</title>

    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());
    </script>

    <!-- Import elegant font -->
    <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:ital@1&display=swap" rel="stylesheet">

    <link href="./website/css" rel="stylesheet">
    <!-- <link rel="stylesheet" href="./website/bootstrap.min.css"> -->
    <link rel="stylesheet" href="./website/bulma.min.css">
    <link rel="stylesheet" href="./website/bulma-carousel.min.css">
    <link rel="stylesheet" href="./website/bulma-slider.min.css">
    <link rel="stylesheet" href="./website/fontawesome.all.min.css">
    <link rel="stylesheet" href="./website/academicons.min.css">
    <link rel="stylesheet" href="./website/index.css">
    <link rel="icon" href="./website/images/dog.gif">

    <script src="./website/select.js"></script>
    <script src="./website/bootstrap.min.js"></script>
    <script src="./website/jquery.min.js.下载"></script>
    <script defer="" src="./website/fontawesome.all.min.js.下载"></script>
    <script src="./website/bulma-carousel.min.js.下载"></script>
    <script src="./website/bulma-slider.min.js.下载"></script>
    <script src="./website/index.js.下载"></script>
    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>


    <style>
        /* General Styles */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Arial', sans-serif;
        }
        
        body {
            color: #333;
            line-height: 1.6;
            padding-top: 0; /* Remove top padding as we're using a full-screen hero */
        }
        
        /* Hero Section with Background Image */
        .hero-background {
            position: relative;
            height: 100vh;
            width: 100%;
            display: flex;
            align-items: center;
            justify-content: center;
            text-align: center;
            color: white;
            overflow: hidden; /* 确保背景不溢出 */
            transition: all 0.5s ease; /* 添加过渡效果 */
        }
        
        /* Background Image Layer */
        .hero-background::before {
            content: "";
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('./website/images/background.png') no-repeat center center/contain;
            filter: brightness(0.4); /* 默认暗色 */
            transition: filter 0.5s ease; /* 过渡效果 */
            z-index: -1;
        }
        
        /* Background hover effect */
        .hero-background:hover::before {
            filter: brightness(1); /* 悬浮时恢复正常亮度 */
        }
        
        /* Dark overlay for text readability */
        .hero-background::after {
            content: "";
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.4); /* 黑色半透明叠加层 */
            z-index: -1;
            transition: background 0.5s ease; /* 过渡效果 */
        }
        
        /* Hover effect for overlay */
        .hero-background:hover::after {
            background: rgba(0, 0, 0, 0.2); /* 降低黑色叠加层的不透明度 */
        }

        /* 悬浮时减少遮罩的不透明度 */
        .hero-background:hover .hero-overlay {
            background: rgba(0, 0, 0, 0.2); /* 降低黑色叠加层的不透明度 */
        }
        
        .hero-content {
            z-index: 2;
            max-width: 800px;
            padding: 20px;
            position: relative;
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        
        /* Elegant title styling */
        .elegant-title {
            font-family: 'Playfair Display', cursive;
            font-size: 5rem;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);
            color: white;
            line-height: 1.2;
            font-weight: normal;
        }
        
        /* Decorative divider */
        .divider {
            width: 80%;
            height: 1px;
            background-color: white;
            margin: 15px auto 20px;
            box-shadow: 0 0 5px rgba(255, 255, 255, 0.5);
        }
        
        /* Subtitle styling */
        .elegant-subtitle {
            font-family: 'Playfair Display', serif;
            font-style: italic;
            font-size: 1.8rem;
            margin-top: 5px;
            margin-bottom: 30px;
            text-shadow: 1px 1px 3px rgba(0, 0, 0, 0.5);
            color: white;
            line-height: 1.4;
        }
        
        /* Scroll down arrow */
        .scroll-down {
            position: absolute;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            font-size: 2rem;
            animation: bounce 2s infinite;
            cursor: pointer;
            z-index: 2;
            color: white;
        }
        
        @keyframes bounce {
            0%, 20%, 50%, 80%, 100% {
                transform: translateY(0) translateX(-50%);
            }
            40% {
                transform: translateY(-20px) translateX(-50%);
            }
            60% {
                transform: translateY(-10px) translateX(-50%);
            }
        }
        
        /* Navigation styling */
        .top-right-nav {
            position: absolute;
            top: 20px;
            right: 20px;
            z-index: 1000;
        }
        
        .top-right-nav ul {
            list-style: none;
            display: flex;
        }
        
        .top-right-nav ul li {
            margin-left: 20px;
        }
        
        .top-right-nav ul li a {
            color: white;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.3s ease;
            padding: 8px 15px;
            border-radius: 5px;
        }
        
        .top-right-nav ul li a:hover {
            background-color: rgba(255, 255, 255, 0.2);
            color: white;
        }
        
        /* Original temp.html styles */
        .header-image {
            width: 60px;
            height: auto;
        }

        .navbar-fixed-top {
            position: fixed;
            top: 0;
            width: 100%;
            z-index: 1000;
            display: none; /* Initially hidden, will be shown after scroll */
        }

        /* Target section padding adjustment for smooth scrolling */
        section:target {
            padding-top: 70px;
            margin-top: -70px;
        }

        ul.nav a:hover {
            color: rgb(255, 249, 239) !important;
        }

        .nav.navbar-nav li a:hover {
            background-color: rgb(82, 127, 159) !important;
        }

        #highlight {
            color: rgb(212, 67, 54);
        }

        .cards {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 30px;
        }
        
        .cards-container {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 20px;
            margin-bottom: 20px;
        }
        
        @media (max-width: 768px) {
            .cards-container {
                grid-template-columns: 1fr;
            }
        }
        
        .card {
            background-color: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            padding: 20px;
            text-align: center;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        
        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
        }
        
        .card.active {
            background-color: #f0f0f0;
            border: 2px solid #2196F3;
        }
        
        .card-title {
            font-size: 18px;
            font-weight: bold;
            margin-bottom: 5px;
        }
        
        .content-area {
            background-color: white;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            padding: 25px;
            margin-top: 30px;
            display: none;
            animation: fadeIn 0.5s ease;
        }

        .content-area.active {
            display: block;
        }
        
        .content-title {
            font-size: 20px;
            font-weight: bold;
            margin-bottom: 20px;
            color: #14191d;
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 10px;
        }
        
        .content-list {
            list-style-position: inside;
            padding-left: 10px;
        }
        
        .content-list li {
            margin-bottom: 12px;
            line-height: 1.5;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(-20px); }
            to { opacity: 1; transform: translateY(0); }
        }

        /* tabs */
        .tabs {
            display: flex;
            justify-content: space-between;
            gap: 15px;
            margin-bottom: 30px;
        }
        .tab {
            flex: 1;
            padding: 20px;
            text-align: center;
            background-color: #f8f8f8;
            border: 1px solid #ddd;
            border-radius: 5px;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        .tab.active {
            background-color: #4a89dc;
            color: white;
            border-color: #4a89dc;
        }
        .tab:hover {
            background-color: #e0e0e0;
        }
        .tab.active:hover {
            background-color: #3a70b7;
        }
        .content-section {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 20px;
            display: none;
        }
        .content-section.active {
            display: block;
        }
        
        /* Table styles for SLM details */
        .slm-table-container {
            max-width: 100%;
            overflow-x: auto;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            font-size: 14px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
            vertical-align: top;
        }
        th {
            background-color: #f2f2f2;
            font-weight: bold;
        }
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        .model-group {
            background-color: #eef7ff;
        }

        caption {
            font-weight: bold;
            margin-bottom: 10px;
            font-size: 16px;
        }
        
        /* Make original content appear below the hero section */
        .original-content {
            margin-top: 0;
            padding-top: 50px;
        }
        
        /* Media queries for responsive design */
        @media (max-width: 768px) {
            .elegant-title {
                font-size: 3.5rem;
            }
            
            .elegant-subtitle {
                font-size: 1.4rem;
            }
            
            .top-right-nav {
                position: relative;
                top: 0;
                right: 0;
                width: 100%;
                text-align: center;
                padding: 10px 0;
                background-color: rgba(0, 0, 0, 0.5);
            }
            
            .top-right-nav ul {
                flex-direction: column;
                align-items: center;
            }
            
            .top-right-nav ul li {
                margin: 5px 0;
            }
        }
    </style>

    <!-- Script to show navbar after scrolling past hero section -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            var heroHeight = document.querySelector('.hero-background').offsetHeight;
            var navbar = document.querySelector('.navbar-fixed-top');
            
            window.addEventListener('scroll', function() {
                if (window.pageYOffset > heroHeight - 70) {
                    navbar.style.display = 'block';
                } else {
                    navbar.style.display = 'none';
                }
            });
        });
    </script>

    <script>
        // 跟踪当前激活的内容区域和卡片
        let activeContent = null;
        let activeCard = null;

        function showContent(contentType) {
            // 获取所有内容区域和卡片
            const allContents = document.querySelectorAll('.content-area');
            const allCards = document.querySelectorAll('.card');
            const targetContent = document.getElementById(`${contentType}-content`);
            const targetCard = document.getElementById(`${contentType}-card`);
            
            // 如果点击的是当前已激活的卡片，则隐藏内容
            if (activeContent === contentType) {
                targetContent.classList.remove('active');
                targetCard.classList.remove('active');
                activeContent = null;
                activeCard = null;
                return;
            }
            
            // 隐藏所有内容区域并重置所有卡片样式
            allContents.forEach(content => content.classList.remove('active'));
            allCards.forEach(card => card.classList.remove('active'));
            
            // 显示目标内容并高亮目标卡片
            targetContent.classList.add('active');
            targetCard.classList.add('active');
            
            // 更新当前激活的内容和卡片
            activeContent = contentType;
            activeCard = targetCard;
        }
    </script>

    <script>
        // 使用 fetch 来加载外部的 LaTeX 文件并渲染它
        window.onload = function() {
        fetch('table.tex') // 指向你的 LaTeX 文件
            .then(response => response.text())
            .then(data => {
            // 将 LaTeX 内容插入到页面中
            document.getElementById('latex-table').innerHTML = `$$ ${data} $$`;
            // 重新渲染 MathJax
            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
            })
            .catch(error => {
            console.error('加载 LaTeX 文件失败:', error);
            });
        }
    </script>
</head>

<body data-new-gr-c-s-check-loaded="14.1147.0" data-gr-ext-installed="">
    <!-- Hero Background Section -->
    <div class="hero-background" id="top">
        <!-- Top-right Navigation -->
        <nav class="top-right-nav">
            <ul>
                <li><a href="#paper_overview">Overview</a></li>
                <li><a href="#results">Results</a></li>
                <li><a href="#examples">Examples</a></li>
                <li><a href="#ethics">Ethics</a></li>
            </ul>
        </nav>
        
        <div class="hero-content">
            <h1 class="elegant-title">SLM Jailbreak</h1>
            <div class="divider"></div>
            <p class="elegant-subtitle">Can Small Language Model Reliably Resist Jailbreak Attack? A Comprehensive Evaluation</p>
        </div>
        
        <div class="scroll-down" onclick="document.getElementById('original-content').scrollIntoView({behavior: 'smooth'})">
            &#8595;
        </div>
    </div>

    <!-- Original content from temp.html -->
    <div id="original-content" class="original-content">
        <section class="hero">
            <div class="hero-body">
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column has-text-centered">
                            <h2 class="title is-2 publication-title">
                                <img src="./website/images/dog.gif" class="header-image"
                                    style="vertical-align: middle; margin-right: 10px;">
                                    Can Small Language Model Reliably Resist Jailbreak Attack? A Comprehensive Evaluation
                            </h2>
                            <div class="is-size-5 publication-authors">
                                <span class="author-block">Zhibo Wang<sup>12</sup>, </span>
                                <span class="author-block">Wenhui Zhang<sup>12</sup>, </span>
                                <span class="author-block">Huiyu Xu<sup>12</sup>, </span>
                                <span class="author-block">Zeqing He<sup>12</sup>, </span>
                                <span class="author-block">Ziqi Zhu<sup>12</sup>, </span>
                            </div>

                            <div class="is-size-5 publication-authors">
                                <span class="author-block"><sup>1</sup>The State Key Laboratory of Blockchain and Data Security, Zhejiang University, P. R. China&emsp;</span>
                                <span class="author-block"><sup>2</sup>School of Cyber Science and Technology, Zhejiang University, P. R. China &emsp;</span>
                            </div>

                            <div class="column has-text-centered">
                                <div class="publication-links">
                                    <!-- PDF Link. -->
                                    <span class="link-block">
                                        <a href="https://arxiv.org/abs/2503.06519"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true"
                                                    focusable="false" data-prefix="fas" data-icon="file-pdf" role="img"
                                                    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"
                                                    data-fa-i2svg="">
                                                    <path fill="currentColor"
                                                        d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z">
                                                    </path>
                                                </svg>
                                            </span>
                                            <span>Paper</span>
                                        </a>
                                    </span>
                                    <!-- Code Link. -->
                                    <span class="link-block">
                                        <a href="https://github.com/Wendy-1222/HarmBench_SLM"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true"
                                                    focusable="false" data-prefix="fab" data-icon="github" role="img"
                                                    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"
                                                    data-fa-i2svg="">
                                                    <path fill="currentColor"
                                                        d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z">
                                                    </path>
                                                </svg>
                                            </span>
                                            <span>Code</span>
                                        </a>
                                    </span>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Add content sections from the original temp.html as needed -->
        <section class="section" id="paper_overview">
            <div class="container is-max-desktop">
                <!-- Abstract. -->
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Overview</h2>
                        <div class="content has-text-justified">
                            <p>
                                This project is about a evaluation of the vulnerabilities of SLMs (Small Language Models) against jailbreak attacks. 
                                
                                <br><br>
    
                                <b>What did we introduce?</b> We develop a framework to systematically assess the vulnerability 
                                of SLMs to jailbreak attacks. This framework encompasses 14 risk categories and 9 attack methods, 
                                and it covers 15 SLM families consists of 62 SLMs, ensuring a thorough and diverse evaluation.
                                <br><br>
                                
                                <b>What did we find?</b> Through systematically evaluation on 62 SLMs from 15 mainstream SLM 
                                families against 9 state-of-the-art jailbreak methods, we demonstrate that <b>51.6%</b> of evaluated 
                                SLMs show high susceptibility to jailbreak attacks (ASR > 40%) and <b>38.7%</b> of them can not even 
                                resist direct harmful query (ASR > 50%). Through correlation analysis, we identify that the 
                                primary factors behind SLM vulnerabilities lie in the training details (e.g., training dataset 
                                and method) rather than model size scaling. Additionally, we evaluate the effectiveness of 
                                mainstream defense methods that can be applied to SLMs and find that none of them achieve 
                                satisfactory results, exhibiting limited performance and poor generalization across different 
                                attack methods. Building upon these findings, we highlight the need for security-by-design approaches 
                                in SLM development and provide valuable insights for building more trustworthy SLM ecosystem.
                                <br><br>
                            </p>
                        </div>
                    </div>
                </div>
                <br>
                <div class="columns is-centered has-text-centered">
                    <div class="container is-max-desktop" style="display: flex; flex-direction: column; align-items: center;">
                        <div>
                            <img src="./website/images/overview.png" alt="" style="width:100%; height:100%;" class="center">
                        </div>
                        <div class="content has-text-centered" style="max-width: 100%;">
                            <p>
                                <br>
                                <span style="font-weight: bold; font-size: larger;">Figure 1. </span>
                                Overview of our evaluation framework. 
                                <!-- We evaluate the jailbreak vulnerabilities of 15 SLM families including 62 SLMs against 9 mainstream jailbreak attack methods with a class-balanced dataset (70 questions). -->
                            </p>
                        </div>
                    </div>
                </div>            
            </div>
        </section>
        
        <section class="section" id="evaluation-details">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-full-width">
                        <h2 class="title is-3">Evaluation Details</h2>
                    </div>
                </div>
                <br>

                <div class="cards-container">
                    <!-- 卡片1 -->
                    <div class="card" id="slms-card" onclick="showContent('slms')">
                        <div class="card-title">SLMs</div>
                    </div>
                    
                    <!-- 卡片2 -->
                    <div class="card" id="jailbreak-card" onclick="showContent('jailbreak')">
                        <div class="card-title">Jailbreak Methods</div>
                    </div>
                    
                    <!-- 卡片3 -->
                    <div class="card" id="defense-card" onclick="showContent('defense')">
                        <div class="card-title">Defense Methods</div>
                    </div>
                </div>
        
                <!-- 内容区域 - SLMs -->
                <div id="slms-content" class="content-area">
                    <h2 class="content-title">Small Language Models (SLMs)</h2>
                    <!-- <div class="tabs">
                        <div class="tab active" data-tab="slms">SLMs</div>
                        <div class="tab" data-tab="jailbreak">Jailbreak Methods</div>
                        <div class="tab" data-tab="defense">Defense Methods</div>
                    </div> -->
                    
                    <div id="slms" class="content-section active">
                        <div class="slm-table-container">
                            <table>
                                <!-- <caption>Details of SLMs used in our experiment. Specifically, we collect 15 SLM families including 62 SLMs released between March 2023 and present, with parameter ranging from 135M to 7B.</caption> -->
                                <thead>
                                    <tr>
                                        <th>Affiliation</th>
                                        <th>Model</th>
                                        <th>Size</th>
                                        <th>Date</th>
                                        <th>Training Datasets</th>
                                        <th>Training Techniques</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <!-- DataBricks -->
                                    <tr class="model-group">
                                        <td rowspan="2">DataBricks</td>
                                        <td>Dolly v1</td>
                                        <td>6B</td>
                                        <td>2023.3</td>
                                        <td>Pile; Stanford Alpaca</td>
                                        <td>RoPE; Fine-tuning; Deepspeed ZeRO 3</td>
                                    </tr>
                                    <tr>
                                        <td>Dolly v2</td>
                                        <td>2.8B; 6.9B</td>
                                        <td>2023.4</td>
                                        <td>Pile; Databricks-dolly-15k</td>
                                        <td>Fine-tuning</td>
                                    </tr>
                    
                                    <!-- StabilityAI -->
                                    <tr class="model-group">
                                        <td rowspan="2">StabilityAI</td>
                                        <td>StableLM</td>
                                        <td>3B</td>
                                        <td>2023.4</td>
                                        <td>RefinedWeb; RedPajama; The Pile; StarCoder</td>
                                        <td>MHA; SiLU; SFT; DPO; RoPE; LayerNorm</td>
                                    </tr>
                                    <tr>
                                        <td>StableLM 2</td>
                                        <td>1.6B</td>
                                        <td>2024.2</td>
                                        <td>RefinedWeb; subsets of the Pile; RedPajama; the Stack; OpenWebText; OpenWebMath; and parts of CulturaX</td>
                                        <td>RoPE; LayerNorm; No Biases; FlashAttention-2; Multi-stage infinite scheduler; SFT; DPO</td>
                                    </tr>
                    
                                    <!-- Alibaba -->
                                    <tr class="model-group">
                                        <td rowspan="4">Alibaba</td>
                                        <td>Qwen 1</td>
                                        <td>1.8B; 7B</td>
                                        <td>2023.9</td>
                                        <td>Unknown</td>
                                        <td>MHA; RoPE; SwiGLU; RMSNorm</td>
                                    </tr>
                                    <tr>
                                        <td>Qwen 1.5</td>
                                        <td>0.5B; 1.8B; 4B; 7B</td>
                                        <td>2024.2</td>
                                        <td>Unknown</td>
                                        <td>MHA; RoPE; SwiGLU; RMSNorm; Multilingual support</td>
                                    </tr>
                                    <tr>
                                        <td>Qwen 2</td>
                                        <td>0.5B; 1.5B; 7B</td>
                                        <td>2024.6</td>
                                        <td>Unknown</td>
                                        <td>GQA; RoPE; SwiGLU; RMSNorm; Multilingual support</td>
                                    </tr>
                                    <tr>
                                        <td>Qwen 2.5</td>
                                        <td>0.5B; 1.5B; 3B; 7B</td>
                                        <td>2024.9</td>
                                        <td>Unknown</td>
                                        <td>GQA; RoPE; SwiGLU; RMSNorm; Multilingual support; Larger corpus</td>
                                    </tr>
                    
                                    <!-- Meituan -->
                                    <tr>
                                        <td>Meituan</td>
                                        <td>MobileLLaMA</td>
                                        <td>1.4B; 2.7B</td>
                                        <td>2023.12</td>
                                        <td>RedPajama v1</td>
                                        <td>RoPE; RMSNorm; SwiGLU; Deep ZERO 1; Flash Attention V2</td>
                                    </tr>
                    
                                    <!-- StabilityAI - TinyLlama -->
                                    <tr>
                                        <td>StabilityAI</td>
                                        <td>TinyLlama v1.1</td>
                                        <td>1.1B</td>
                                        <td>2024.1</td>
                                        <td>SlimPajama</td>
                                        <td>RoPE; RMSNorm; SwiGLU; GQA; Flash Attention; xFormers</td>
                                    </tr>
                    
                                    <!-- H2O -->
                                    <tr class="model-group">
                                        <td rowspan="3">H2O</td>
                                        <td>H2O-Danube</td>
                                        <td>1.8B</td>
                                        <td>2024.1</td>
                                        <td>Unknown</td>
                                        <td>RoPE; GQA; RMSNorm; Sliding window</td>
                                    </tr>
                                    <tr>
                                        <td>H2O-Danube2</td>
                                        <td>1.8B</td>
                                        <td>2024.4</td>
                                        <td>Unknown</td>
                                        <td>RoPE; GQA; RMSNorm; three training stages with different data mixes</td>
                                    </tr>
                                    <tr>
                                        <td>H2O-Danube3</td>
                                        <td>500M; 4B</td>
                                        <td>2024.7</td>
                                        <td>Unknown</td>
                                        <td>RoPE; GQA; RMSNorm; three training stages with different data mixes</td>
                                    </tr>
                    
                                    <!-- AllenAI -->
                                    <tr>
                                        <td>AllenAI</td>
                                        <td>OLMo</td>
                                        <td>7B</td>
                                        <td>2024.2</td>
                                        <td>Dolma</td>
                                        <td>SwiGLU; RoPE; Non-parameteric Layer Norm; No biases; ZeRO</td>
                                    </tr>
                    
                                    <!-- MBZUAI -->
                                    <tr>
                                        <td>MBZUAI</td>
                                        <td>MobiLlama</td>
                                        <td>0.5B; 1.2B</td>
                                        <td>2024.2</td>
                                        <td>LLM360 Amber (includes Arxiv, Book, C4, Refined-Web, StarCoder, StackExchange, and Wikipedia)</td>
                                        <td>RoPE; SwiGLU; RMSNorm; Parameter-sharing; FlashAttention</td>
                                    </tr>
                    
                                    <!-- Google -->
                                    <tr class="model-group">
                                        <td rowspan="3">Google</td>
                                        <td>Gemma</td>
                                        <td>2B; 7B</td>
                                        <td>2024.3</td>
                                        <td>Unknown</td>
                                        <td>MHA; RoPE; GeGLU; RMSNorm; SFT; RLHF</td>
                                    </tr>
                                    <tr>
                                        <td>Gemma 1.1</td>
                                        <td>2B; 7B</td>
                                        <td>2024.3</td>
                                        <td>Unknown</td>
                                        <td>MHA; RoPE; GeGLU; RMSNorm; SFT; RLHF</td>
                                    </tr>
                                    <tr>
                                        <td>Gemma 2</td>
                                        <td>2B</td>
                                        <td>2024.7</td>
                                        <td>Unknown</td>
                                        <td>GQA; RoPE; GeGLU; Local Sliding Window and Global Attention; Logit Soft-Capping; RMSNorm for Pre and Post-Normalization</td>
                                    </tr>
                    
                                    <!-- Tsinghua Univ -->
                                    <tr class="model-group">
                                        <td rowspan="2">Tsinghua Univ.</td>
                                        <td>MiniCPM</td>
                                        <td>1.2B; 2.4B</td>
                                        <td>2024.4</td>
                                        <td>Dolma; C4; Pile; stack; StarCoder; UltraChat; OssInstruct; EvolInstruct</td>
                                        <td>Warmup-Stable-Decay (WSD) learning rate scheduler (LRS); SFT; DPO; Embedding Sharing; GQA</td>
                                    </tr>
                                    <tr>
                                        <td>MiniCPM3</td>
                                        <td>4B</td>
                                        <td>2024.9</td>
                                        <td>Unknown</td>
                                        <td>Unknown</td>
                                    </tr>
                    
                                    <!-- Microsoft -->
                                    <tr class="model-group">
                                        <td rowspan="2">Microsoft</td>
                                        <td>Phi-3</td>
                                        <td>3.8B</td>
                                        <td>2024.4</td>
                                        <td>Scaled-up dataset from phi-2</td>
                                        <td>MHA; SiLU; RoPE; FlashAttention; Deep ZeRO Stage 2</td>
                                    </tr>
                                    <tr>
                                        <td>Phi-3.5</td>
                                        <td>3.8B</td>
                                        <td>2024.4</td>
                                        <td>more multilingual and long-text data</td>
                                        <td>Multilingual; MHA; SiLU; RoPE; FlashAttention; ZeRO 2</td>
                                    </tr>
                    
                                    <!-- TensorOpera -->
                                    <tr>
                                        <td>TensorOpera</td>
                                        <td>Fox-1</td>
                                        <td>1.6B</td>
                                        <td>2024.6</td>
                                        <td>Unknown</td>
                                        <td>RMSNorm; RoPE; GQA; Deep architecture; Shared Embedding</td>
                                    </tr>
                    
                                    <!-- HuggingFace -->
                                    <tr class="model-group">
                                        <td rowspan="2">HuggingFace</td>
                                        <td>SmolLM</td>
                                        <td>135M; 360M; 1.7B</td>
                                        <td>2024.7</td>
                                        <td>SmolLM-Corpus</td>
                                        <td>GQA; trapezoidal LR scheduler</td>
                                    </tr>
                                    <tr>
                                        <td>SmolLM2</td>
                                        <td>135M; 360M; 1.7B</td>
                                        <td>2025.2</td>
                                        <td>Cosmopedia v2; FineWeb-Edu; Stack-Edu; FineMath; DCLM</td>
                                        <td>Multi-stage training; Warmup-Stable-Decay (WSD) learning rate scheduler</td>
                                    </tr>
                    
                                    <!-- Meta -->
                                    <tr>
                                        <td>Meta</td>
                                        <td>Llama 3.2</td>
                                        <td>1B; 3B</td>
                                        <td>2024.9</td>
                                        <td>Unknown</td>
                                        <td>GQA; SiLU; Shared Embeddings; Pruning; Knowledge distillation; SFT; RLHF; RS; DPO</td>
                                    </tr>
                    
                                    <!-- DeepSeek -->
                                    <tr>
                                        <td>DeepSeek</td>
                                        <td>DeepSeek-R1</td>
                                        <td>1.5B; 7B</td>
                                        <td>2025.1</td>
                                        <td>Unknown</td>
                                        <td>SFT; RL; Knowledge distillation</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>
                </div>
        
                <!-- 内容区域 - Jailbreak Methods -->
                <div id="jailbreak-content" class="content-area">
                    <h2 class="content-title">Jailbreak Methods</h2>
                    <ul class="content-list">
                        <li><strong><a href="" style="color:rgb(0, 115, 255)">HumanJailbreaks</a></strong> - This method uses the vanilla harmful query to test whether the model can identify the malicious intent and reject to engage in policy-violating activities such as making a bomb.</li>
                        <li><strong><a href="https://arxiv.org/abs/2308.03825" style="color:rgb(0, 115, 255)">HumanJailbreaks</a></strong> - This methods uses a fixed set of in-the-wild jailbreak template that can be embedded into any harmful question without additional transformation or optimization.</li>
                        <li><strong><a href="https://arxiv.org/abs/2310.04451" style="color:rgb(0, 115, 255)">AutoDAN</a></strong> - This method initialized with handcrafted jailbreak templates as seeds and then optimize these seed using a hierarchical genetic algorithm so as to elicit targeted behaviors from the target model. 。</li>
                        <li><strong><a href="https://arxiv.org/abs/2401.06373" style="color:rgb(0, 115, 255)">PAP</a></strong> - This method treats the target model as a human-like communicator and use persuasive strategies to induce the target model to answer harmful queries. Specifically, they fine-tune a GPT-3.5-turbo to craft jailbreak prompts according to each persuasive strategy.</li>
                        <li><strong><a href="https://arxiv.org/abs/2307.15043" style="color:rgb(0, 115, 255)">GCG</a></strong> - This method optimize an adversarial suffix using a combination of greedy search and gradient-based optimization from random initialization. The suffix is then appended to the harmful query to maximize the likelihood that the target model respond with the expected prefix (e.g., ``Sure, here is a tutorial for how to make a bomb......").</li>
                        <li><strong><a href="https://arxiv.org/abs/2010.15980" style="color:rgb(0, 115, 255)">AutoPrompt</a></strong> - This method is similar to GCG and also uses a gradient-based approach to optimize an adversarial suffix, but AutoPrompt focuses on adjusting a single coordinate (token) at a time, optimizing it based on gradient information before moving on to the next token. This sequential approach, while effective, is less comprehensive than GCG's method of considering all possible token replacements at each step.</li>
                        <li><strong><a href="https://arxiv.org/abs/2302.03668" style="color:rgb(0, 115, 255)">PEZ</a></strong> - This method uses a gradient-based method to optimize hard text prompts. Specifically, it combines the ease of soft prompt optimization and the interpretability of hard prompts by maintaining continuous embeddings during optimization and projecting them onto the nearest discrete tokens.</li>
                        <li><strong><a href="https://arxiv.org/abs/2104.13733" style="color:rgb(0, 115, 255)">GBDA</a></strong> - This method leverages the Gumbel-softmax distribution to optimize a distribution of potential adversarial suffixes. This allows for gradient-based optimization and incorporates differentiable constraints such as BERTScore and language model perplexity to ensure fluency and semantic similarity.</li>
                        <li><strong><a href="https://arxiv.org/abs/1908.07125" style="color:rgb(0, 115, 255)">UAT</a></strong> - This method iteratively update the embedding for every trigger token to minimizes the target loss’ first-order Taylor approximation around the current token embedding.</li>
                    </ul>
                </div>

                <!-- 内容区域 - Defense Methods -->
                <div id="defense-content" class="content-area">
                    <h2 class="content-title">Defense Methods</h2>
                    <ul class="content-list">
                        <li><strong><a href="https://arxiv.org/abs/2308.14132" style="color:rgb(0, 115, 255)">PPL</a></strong> - Perplexity is a metric used to assess the accuracy of a language model in predicting the next word. When the perplexity of a model abnormally increases, it indicates that the input contains unnatural linguistic structures that the model has not learned (e.g., adversarial suffix). As a result, PPL-based defense method sets a perplexity threshold and filters out input prompts whose perplexity is higher than the threshold.</li>
                        <li><strong><a href="https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard3/1B/MODEL_CARD.md" style="color:rgb(0, 115, 255)">Llama Guard 3-1B</a></strong> - Llama Guard 3-1B, developed by Meta, is a fine-tuned Llama-3.2-1B pretrained model for content safety classification. Given a text, the model generates output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated. </li>
                        <li><strong><a href="https://arxiv.org/abs/2309.00614" style="color:rgb(0, 115, 255)">Retokenization</a></strong> - Tokenization is the process of converting a text prompt into a sequence of tokens that the language model can process (e.g., words, subwords, or characters), while the Retokenization defense retokenizes the input prompt using alternative schemes, altering boundaries or adding noise, thus disrupt the jailbreak features.</li>
                        <li><strong><a href="https://www.nature.com/articles/s42256-023-00765-8" style="color:rgb(0, 115, 255)">Self-Reminder</a></strong> - This defense is inspired by the psychological concept of self-reminders, and add safety reminders in the system prompt of target model to induce the model to respond responsibly.</li>
                    </ul>
                </div>
            </div>
        </section>


        <section class="section" id="results">
            <!-- Benchmark design. -->
    
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-full-width">
                        <h2 class="title is-3">Results</h2>
                    </div>
                </div>
                <br>
    
                <div class="content has-text-centered">
                    <h2 class="title is-4">Main Results</h2>
                    <div class="content has-text-justified">
                        <p>
                            Through a comprehensive evaluation of current SLMs against jailbreak attacks, we find that SLMs exhibit limited safety in the face of jailbreak attacks. Half of evaluated SLMs are highly vulnerable to jailbreak attacks, with average attack success rate~(ASR) greater than 40\%, and 38.7\% of them even show an ASR of more than 50\% when faced with direct harmful query.
                            Only a small number of SLMs demonstrate relatively good resistance to jailbreak attacks, such as Google's Gemma series and Microsoft's Phi-3 series.
                            
                            <br><br>
                            Among these results, we observe that SLMs exhibit imbalanced vulnerabilities when facing different attack methods and risk categories. Notably, we find that current SLMs are more vulnerable to advanced, optimization-based attacks~(e.g., GCG~\cite{zou2023GCG} and AutoDAN~\cite{liu2023autodan}) and are highly susceptible to the ``Economic Harm'' risk category~(ASR $\approx$ 100\%), while showing greater safety in the ``Health Consultation'' risk category~(ASR < 20\%). 
                        </p>
                    </div>
                </div>
    
                <br>
    
                <div class="content has-text-centered">
                    <h2 class="title is-4">Factor Analysis</h2>
                    <div class="content has-text-justified">
                        <p>
                            After conducting correlation tests between potential factors and the vulnerability of SLMs, we identify several key factors that significantly impact their susceptibility. 
                            
                            <br><br>
                            Contrary to common intuition, we find that the robustness of SLMs is more influenced by training details than by the model size itself. Interestingly, scaling up the model size and training dataset does not always enhance model safety and can sometimes have an adverse effect.
                            For example, expanding the model's corpus may increase the risk of jailbreak attacks, while moderate degree of sparse techniques may improve robustness. 
                            
                            <br><br>
                            Notably, supervised fine-tuning~(SFT) models demonstrate 10\textasciitilde 40\% better robustness compared to their DPO-optimized~(Direct Preference Optimization) counterparts at most time. In addition, through evaluating diversity and fluency of jailbreak responses, we found that some SLMs~(e.g., the TinyLlama family) may output repetitive harmful sentences due to limited generation capability. 
                        </p>
                    </div>
    
                    <!-- <img src="./website/images/ablation_study.png" alt=""> -->
                </div>
    
                <br>
    
                <div class="content has-text-centered">
                    <h3 class="title is-4">Defense Effectiveness</h3>
                    <div class="content has-text-justified">
                        <p>
                            we test the effectiveness of four lightweight defense methods in SLMs and found that none of them could achieve perfect performance in all SLMs across different attack methods. This reveals the limitations of existing defense measures and underscores the urgent need for SLM developers to place greater emphasis on security during SLMs design and training.

                        </p>
                    </div>
                    
                    <!-- <div class="columns is-centered">
                        <div class="column">
                            <img src="./website/images/illegal_category_strategy_heatmap_gpt-3.5-turbo-1106_14_category_70.png" alt="">
                        </div>
                        <div class="column">
                            <img src="./website/images/illegal_category_strategy_heatmap_vicuna-7b_14_category_70.png" alt="">
                        </div>
                    </div> -->
                </div>            
    
            </div>
        </section>

    
    
    
        <section class="section" id="ethics">
            <!-- Benchmark design. -->
    
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="container is-max-desktop">
                        <h2 class="title is-3">Ethics and Disclosure</h2>
                        <div class="content has-text-justified">
                            <p>
                                In this study, we adhered to ethical standards to ensure safety and privacy. Our experiments were conducted 
                                using platforms provided officially or through open-source models deployed in a closed environment. 
                                We did not disseminate any harmful or illicit content to the public or others. The datasets we employed 
                                were obtained from public repositories and did not contain any personal information. The main objective of 
                                this study is to highlight potential vulnerabilities in LLMs, especially given the rapid pace of their adoption. 
                                Moreover, we have responsibly disclosed our findings to OpenAI and Meta.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    
        <section class="section" id="BibTeX">
            <div class="container is-max-desktop content">
                <h2 class="title">BibTeX</h2>
                <p>
                    If you find our project useful, please consider citing:
                </p>
                <pre><code>@article{zhang2025can,
                    title={Can Small Language Models Reliably Resist Jailbreak Attacks? A Comprehensive Evaluation},
                    author={Zhang, Wenhui and Xu, Huiyu and Wang, Zhibo and He, Zeqing and Zhu, Ziqi and Ren, Kui},
                    journal={arXiv preprint arXiv:2503.06519},
                    year={2025}
                  }
    }</code></pre>
            </div>
        </section>   
    
        <footer class="footer">
            <div class="container">
                <div class="columns is-centered">
                    <div class="column is-8">
                        <div class="content">
                            <p>
                                This website is borrowed from <a href="https://chats-lab.github.io/persuasive_jailbreaker/">PAP
                                </a>, <a href="https://nerfies.github.io/">Nerfies</a>,
                                <a href="https://jailbreaking-llms.github.io/">PAIR</a>,
                                and licensed under a <a rel="license"
                                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                                    Commons Attribution-ShareAlike 4.0 International License</a>.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </footer>
    
    
    </body>
    <grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open">
            <style>
                div.grammarly-desktop-integration {
                    position: absolute;
                    width: 1px;
                    height: 1px;
                    padding: 0;
                    margin: -1px;
                    overflow: hidden;
                    clip: rect(0, 0, 0, 0);
                    white-space: nowrap;
                    border: 0;
                    -moz-user-select: none;
                    -webkit-user-select: none;
                    -ms-user-select: none;
                    user-select: none;
                }

                div.grammarly-desktop-integration:before {
                    content: attr(data-content);
                }
            </style>
            <div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration"
                data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}">
            </div>
        </template></grammarly-desktop-integration>
    
    
</html>