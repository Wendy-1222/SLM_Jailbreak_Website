\begin{table*}[ht!]
\centering
\caption{Details of SLMs used in our experiment. Specifically, we collect 15 SLM families including 62 SLMs released between March 2023 and present, with parameter ranging from 135M to 7B. To enable nuanced comparison, we also list their training datasets and techniques, which are collected from model cards and technical reports.}
\vspace{-5pt}
\tiny
\renewcommand{\arraystretch}{1.4}
% \resizebox{.99\linewidth}{!}{
\begin{tabular}{l|p{2cm}|p{1.8cm}|p{0.4cm}|p{4cm}|p{5cm}}
\hline
\textbf{Affiliation} & \textbf{Model} & \textbf{Size} & \textbf{Date} & \textbf{Training Datasets} & \textbf{Training Techniques} \\
\hline

\multirow{2}{*}{DataBricks ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/databricks.png}}} & Dolly v1 \cite{DatabricksBlog2023DollyV1} & 6B & 2023.3 & Pile~\cite{gao2020pile}; Stanford Alpaca~\cite{alpaca} & RoPE; Fine-tuning; Deepspeed ZeRO 3 \\
\cline{2-6}
 & Dolly v2 \cite{DatabricksBlog2023DollyV2} & 2.8B; 6.9B & 2023.4 & Pile~\cite{gao2020pile}; Databricks-dolly-15k~\cite{DatabricksBlog2023DollyV2} & Fine-tuning \\
\hline

\multirow{2}{*}{StabilityAI ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/stabilityai.png}}} & StableLM~\cite{stabilityai2023stablelm}& 3B & 2023.4 & RefinedWeb~\cite{penedo2023refinedweb}; RedPajama~\cite{together2023redpajama}; The Pile~\cite{gao2020pile}; StarCoder~\cite{li2023starcoder} & MHA; SiLU; SFT; DPO; RoPE; LayerNorm \\
\cline{2-6}
& StableLM 2~\cite{bellagente2024stable}& 1.6B & 2024.2 & RefinedWeb \cite{penedo2023refinedweb}; subsets of the Pile~\cite{gao2020pile}; RedPajama~\cite{together2023redpajama}; the Stack \cite{kocetkov2022stack}; OpenWebText \cite{gokaslan2019openwebtext}; OpenWebMath \cite{paster2024openwebmath}; and parts of CulturaX \cite{nguyen2024culturax} & RoPE; LayerNorm; No Biases; FlashAttention-2; Multi-stage infinite scheduler; SFT; DPO \\
\hline

\multirow{5}{*}{Alibaba ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/alibaba.jpg}}} & Qwen 1~\cite{bai2023qwentechnicalreport} & 1.8B; 7B & 2023.9 & Unknown & MHA; RoPE; SwiGLU; RMSNorm\\
\cline{2-6}
 & Qwen 1.5~\cite{bai2023qwentechnicalreport} & 0.5B; 1.8B; 4B; 7B & 2024.2 & Unknown & MHA; RoPE; SwiGLU; RMSNorm; Multilingual support \\
\cline{2-6}
 & Qwen 2~\cite{yang2024qwen2} & 0.5B;1.5B; 7B & 2024.6 & Unknown & GQA; RoPE; SwiGLU; RMSNorm; Multilingual support \\
\cline{2-6}
 & Qwen 2.5~\cite{yang2024qwen2} & 0.5B; 1.5B; 3B; 7B & 2024.9 & Unknown & GQA; RoPE; SwiGLU; RMSNorm; Multilingual support; Larger corpus \\
\hline

Meituan ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/meituan.png}} & MobileLLaMA \cite{chu2023mobilevlm} & 1.4B; 2.7B & 2023.12 & RedPajama v1~\cite{together2023redpajama} & RoPE; RMSNorm; SwiGLU; Deep ZERO 1; Flash Attention V2 \\
\hline
StabilityAI ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/stabilityai.png}} & TinyLlama v1.1 \cite{zhang2024tinyllama} & 1.1B & 2024.1 & SlimPajama \cite{cerebras2023slimpajama} & RoPE; RMSNorm; SwiGLU; GQA; Flash Attention; xFormers \\
\hline

\multirow{3}{*}{H2O ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/h2o.jpeg}}} & H2O-Danube \cite{singer2024h2o} & 1.8B & 2024.1 & Unknown & RoPE; GQA; RMSNorm; Sliding window \\
\cline{2-6}
& H2O-Danube2 \cite{singer2024h2o} & 1.8B & 2024.4 & Unknown & RoPE; GQA; RMSNorm; three training stages with different data mixes \\
\cline{2-6}
& H2O-Danube3 \cite{pfeiffer2024h2o3} & 500M; 4B & 2024.7 & Unknown & RoPE; GQA; RMSNorm; three training stages with different data mixes \\
\hline
% & H2O-Danube2 \cite{singer2024h2o} & 1.8B & 2024.4 & Unknown & Three different training stages with different data mixes \\
% \cline{2-6}
% & H2O-Danube3 \cite{pfeiffer2024h2o3} & 500M; 4B & 2024.7 & Unknown & Three different training stages with different data mixes \\
% \hline
AllenAI ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/allenai.png}} & OLMo \cite{groeneveld2024olmo} & 7B & 2024.2 & Dolma \cite{soldaini2024dolma} & SwiGLU; RoPE; Non-parameteric Layer Norm; No biases; ZeRO  \\
\hline

MBZUAI ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/mbzuai.png}} & MobiLlama \cite{thawakar2024mobillama} & 0.5B; 1.2B & 2024.2 & 
LLM360 Amber~\cite{liu2023llm360} (LLM360 Amber includes Arxiv, Book, C4, Refined-Web, StarCoder, StackExchange, and Wikipedia) & RoPE; SwiGLU; RMSNorm; Parameter-sharing; FlashAttention \\
\hline

\multirow{4}{*}{Google ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/google.png}}} & Gemma \cite{team2024gemma} & 2B; 7B & 2024.3 & Unknown & MHA; RoPE; GeGLU; RMSNorm; SFT; RLHF \\
\cline{2-6}
& Gemma 1.1 \cite{team2024gemma} & 2B; 7B & 2024.3 & Unknown & MHA; RoPE; GeGLU; RMSNorm; SFT; RLHF \\
\cline{2-6}
% & RecurrentGemma \cite{botev2024recurrentgemma} & 2B & 2024.4 & Unknown (2T tokens) & Unknown\\
% \cline{2-6}
 & Gemma 2 \cite{team2024gemma2} & 2B & 2024.7 & Unknown & GQA; RoPE; GeGLU; Local Sliding Window and Global Attention; Logit Soft-Capping; RMSNorm for Pre and Post-Normalization \\
\hline

\multirow{2}{*}{Tsinghua Univ. ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/thu.png}}} & MiniCPM \cite{hu2024minicpm} & 1.2B; 2.4B & 2024.4 & Dolma \cite{soldaini2024dolma}; C4 \cite{raffel2020C4}; Pile \cite{gao2020pile}; stack \cite{kocetkov2022stack}; StarCoder \cite{li2023starcoder}; UltraChat \cite{ding2023UltraChat}; OssInstruct \cite{wei2023magicoder}; EvolInstruct \cite{xu2023wizardlm} & Warmup-Stable-Decay (WSD) learning rate scheduler (LRS); SFT; DPO; Embedding Sharing; GQA \\
\cline{2-6}
& MiniCPM3 \cite{hu2024minicpm} & 4B & 2024.9 & Unknown & Unknown\\
\hline

\multirow{2}{*}{Microsoft ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/microsoft.png}}} & Phi-3 \cite{abdin2024phi} & 3.8B & 2024.4 & Scaled-up dataset from phi-2 & MHA; SiLU; RoPE; FlashAttention; Deep ZeRO Stage 2  \\
\cline{2-6}
 & Phi-3.5 \cite{abdin2024phi} & 3.8B & 2024.4 & more multilingual and long-text data & Multilingual; MHA; SiLU; RoPE; FlashAttention; ZeRO 2  \\
\hline

TensorOpera ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/tensoropera.jpeg}} & Fox-1 \cite{fox1} & 1.6B & 2024.6 & Unknown & RMSNorm; RoPE; GQA; Deep architecture; Shared Embedding  \\
% TensorOpera ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/tensoropera.jpeg}} & Fox-1 \cite{fox1} & 1.6B & 2024.6 & Unknown & RMSNorm; RoPE; GQA; Deep architecture; Shared Embedding; Warmup-Stable-Decay (WSD) learning rate scheduler  \\
\hline

% \multirow{2}{*}{HuggingFace ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/huggingface.png}}} & SmolLM \cite{allal2024SmolLM} & 135M; 360M; 1.7B & 2024.7 & SmolLM-Corpus~(including Cosmopedia v2, Python-Edu, FineWeb-Edu (deduplicated)) \cite{benallal2024smollmcorpus} & GQA; trapezoidal LR scheduler \\
\multirow{2}{*}{HuggingFace ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/huggingface.png}}} & SmolLM \cite{allal2024SmolLM} & 135M; 360M; 1.7B & 2024.7 & SmolLM-Corpus \cite{benallal2024smollmcorpus} & GQA; trapezoidal LR scheduler \\
\cline{2-6}
& SmolLM2 \cite{allal2025smollm2} & 135M; 360M; 1.7B & 2025.2 & Cosmopedia v2~\cite{allal2024SmolLM}; FineWeb-Edu~\cite{penedo2024fineweb}; Stack-Edu~\cite{allal2025smollm2}; FineMath~\cite{allal2025smollm2}; DCLM~\cite{li2024DCLM-baseline} & Multi-stage training; Warmup-Stable-Decay (WSD) learning rate scheduler  \\
\hline 

Meta ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/meta.png}} & Llama 3.2~\cite{llama3.2} & 1B; 3B & 2024.9 & Unknown & GQA; SiLU; Shared Embeddings; Pruning; Knowledge distillation; SFT; RLHF; RS; DPO \\
\hline

DeepSeek ~\centering \raisebox{-0.2\height}{\includegraphics[width=0.3cm]{logos/deepseek.png}} & DeepSeek-R1~\cite{guo2025deepseek} & 1.5B; 7B & 2025.1 & Unknown & SFT; RL; Knowledge distillation \\
\hline

% Apple & OpenELM \cite{mehta2024openelm} & 270M; 450M; 1.1B; 3B & 2024.4 & RefinedWeb \cite{penedo2023refinedweb}, deduplicated PILE \cite{gao2020pile}, partial RedPajama \cite{together2023redpajama}, partial Dolma v1.6 \cite{dolma} & No biases in FC layers; Pre-norm: RMSNorm; Pos encoding: RoPE; Attention: GQA; FFN: SwiGLU; Tokenizer: LLaMA-style \\
% \hline

% MBZUAI & Rene \cite{Rene} & 1.3B & 2024.5 & Dolma-1.7 \cite{dolma} & Mamba-2 layers, sliding-window attention (SWA) \\
% \hline

% Tencent & CT-LLM \cite{du2024chinesetinyllmpretraining} & 2B & 2024.4 & MAP-CC & Chinese, MHA, RoPE, SwiGLU, RMSNorm \\  
% \hline 

% Toyota & DCLM \cite{} & 1.4B & 2024.7 & DCLM-Baseline \cite{} & Unknown \\
% \hline

% Cerebras & Cerebras-GPT \cite{dey2023cerebras} & 111M; 256M; 590M; 1.3B; 2.7B; 6.7B & 2023.4 & Pile \cite{gao2020pile} & MHA; GELU; Maximal Update Parameterization \\
% \hline

% EleutherAI & Pythia \cite{biderman2023pythiasuiteanalyzinglarge} & 14M;70M;160M;410M; 1B;1.4B;2.8B;6.9B;> & 2023.4 & Pile \cite{gao2020pile} & MHA; GELU; Flash Attention \cite{dao2022flashattention}; RoPE \cite{su2024roformer}; ZeRO \cite{rajbhandari2020zero} \\
% \hline
\end{tabular} 
% }
% \vspace{13pt}
% \caption{Details of SLMs used in our experiment.}
\vspace{-5pt}
\label{tab:slm_details}
\end{table*}